{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "# for tokenizing\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "  \"\"\"Return a list of tokenized sentence, and\n",
    "      a dictionary of frequency of word\n",
    "    \n",
    "    Args:\n",
    "      path: the path of the file (in txt) to parse and tokenize\n",
    "  \"\"\"\n",
    "  f = open(path, \"r\")\n",
    "  text = f.read()\n",
    "  token_dict = dict()\n",
    "\n",
    "  # documents -> list of sentences\n",
    "  sentences_lst = nltk.sent_tokenize(text)\n",
    "\n",
    "  # list of sentences -> list of tokenized sentences\n",
    "  tokenized_sentences = []\n",
    "  for sentence in sentences_lst:\n",
    "    tokenized = nltk.word_tokenize(sentence)\n",
    "    tokenized_sentences.append(tokenized)\n",
    "    for token in tokenized:\n",
    "      if token in token_dict:\n",
    "        token_dict[token] += 1\n",
    "      else:\n",
    "        token_dict[token] = 1\n",
    "\n",
    "  return tokenized_sentences, token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dict(dict1, dict2):\n",
    "  \"\"\"Return a new dictionary with the content of dict1 and dict2\"\"\"  \n",
    "  res = {**dict1, **dict2}\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    \"\"\"Retun a list of tokenized sentences and a word frequency dictionary\"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    categories = [\"/businessEconomy\", \"/lifestyle\", \"/politics\", \"/technology\"]\n",
    "\n",
    "    tokenized_sentences = []\n",
    "    word_freq_dict = dict()\n",
    "\n",
    "    for category in categories:\n",
    "        docs_list = os.listdir(cwd+category)\n",
    "        for docs in docs_list:\n",
    "            doc_path = cwd+category+\"/\"+docs\n",
    "            tkn_sent, tkn_dict = load_data(doc_path)\n",
    "            tokenized_sentences += tkn_sent\n",
    "            word_freq_dict = merge_dict(word_freq_dict, tkn_dict)\n",
    "\n",
    "    return tokenized_sentences, word_freq_dict        \n",
    "        "
   ]
  }
 ]
}