{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "\n",
    "# for tokenizing\n",
    "nltk.download('punkt')\n",
    "\n",
    "# for POS tagging\n",
    "from nltk.tag.perceptron import AveragedPerceptron\n",
    "from nltk.tag.perceptron import PerceptronTagger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "  \"\"\"Return a list of tokenized sentence, and\n",
    "      a dictionary of frequency of word\n",
    "    \n",
    "    Args:\n",
    "      path: the path of the file (in txt) to parse and tokenize\n",
    "  \"\"\"\n",
    "  f = open(path, \"r\")\n",
    "  text = f.read()\n",
    "  token_dict = dict()\n",
    "\n",
    "  # documents -> list of sentences\n",
    "  sentences_lst = nltk.sent_tokenize(text)\n",
    "\n",
    "  # list of sentences -> list of tokenized sentences\n",
    "  tokenized_sentences = []\n",
    "  for sentence in sentences_lst:\n",
    "    tokenized = nltk.word_tokenize(sentence)\n",
    "    tokenized_sentences.append(tokenized)\n",
    "    for token in tokenized:\n",
    "      if token in token_dict:\n",
    "        token_dict[token] += 1\n",
    "      else:\n",
    "        token_dict[token] = 1\n",
    "\n",
    "  return tokenized_sentences, token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dict(dict1, dict2):\n",
    "  \"\"\"Return a new dictionary with the content of dict1 and dict2\"\"\"  \n",
    "  res = {**dict1, **dict2}\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    \"\"\"Retun a list of tokenized sentences and a word frequency dictionary\"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    categories = [\"/businessEconomy\", \"/lifestyle\", \"/politics\", \"/technology\"]\n",
    "\n",
    "    tokenized_sentences = []\n",
    "    word_freq_dict = dict()\n",
    "\n",
    "    for category in categories:\n",
    "        docs_list = os.listdir(cwd+category)\n",
    "        for docs in docs_list:\n",
    "            doc_path = cwd+category+\"/\"+docs\n",
    "            tkn_sent, tkn_dict = load_data(doc_path)\n",
    "            tokenized_sentences += tkn_sent\n",
    "            word_freq_dict = merge_dict(word_freq_dict, tkn_dict)\n",
    "\n",
    "    return tokenized_sentences, word_freq_dict        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences, word_freq_dict = process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = tf.keras.utils.get_file(fname=\"tagged_tsv_gh\", origin=\"https://raw.githubusercontent.com/famrashel/idn-tagged-corpus/master/Indonesian_Manually_Tagged_Corpus_ID.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PrepareTrainData():\n",
    "    def __init__(self, path):\n",
    "        self.tagged_data = path\n",
    "        self.pos_tags = dict()\n",
    "        self.train_data = []\n",
    "        self.pos_tags_set = set()\n",
    "\n",
    "    def load(self):\n",
    "        f = open(self.tagged_data, \"r\")\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line[:2] == \"</\":\n",
    "                # end of a sentence\n",
    "                self.train_data.append(tagged_entry)\n",
    "            elif line[:1] == \"<\":\n",
    "                # instantiate a new tagged entry\n",
    "                tagged_entry = []\n",
    "            else:\n",
    "                lst = line.split(\"\\t\")\n",
    "                word = lst[0]\n",
    "                tag = lst[1]\n",
    "\n",
    "                # add entry to tagged_entry to build tagged sentence\n",
    "                entry = (word, tag)\n",
    "                tagged_entry.append(entry)\n",
    "                \n",
    "                # add the tag and word to pos_tags\n",
    "                if tag not in self.pos_tags:\n",
    "                    self.pos_tags[tag] = set(word)\n",
    "                else:\n",
    "                    self.pos_tags[tag].add(word)\n",
    "\n",
    "        self.pos_tags_set = set(pos_tags.keys())\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = PrepareTrainData(tagged_data)\n",
    "train_data.load()\n",
    "train_data = train_data.train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_tagger = PerceptronTagger(load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_tagger.train(train_data, save_loc=\"trained_model.pickle\")"
   ]
  }
 ]
}