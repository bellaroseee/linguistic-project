{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# for tokenizing\n",
    "nltk.download('punkt')\n",
    "\n",
    "# for POS tagging\n",
    "from nltk.tag.perceptron import AveragedPerceptron\n",
    "from nltk.tag.perceptron import PerceptronTagger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "  \"\"\"Return a list of tokenized sentence, and\n",
    "      a dictionary of frequency of word\n",
    "    \n",
    "    Args:\n",
    "      path: the path of the file (in txt) to parse and tokenize\n",
    "  \"\"\"\n",
    "  f = open(path, \"r\")\n",
    "  text = f.read()\n",
    "  token_dict = dict()\n",
    "\n",
    "  # documents -> list of sentences\n",
    "  sentences_lst = nltk.sent_tokenize(text)\n",
    "\n",
    "  # list of sentences -> list of tokenized sentences\n",
    "  tokenized_sentences = []\n",
    "  for sentence in sentences_lst:\n",
    "    tokenized = nltk.word_tokenize(sentence)\n",
    "    tokenized_sentences.append(tokenized)\n",
    "    for token in tokenized:\n",
    "      if token in token_dict:\n",
    "        token_dict[token] += 1\n",
    "      else:\n",
    "        token_dict[token] = 1\n",
    "\n",
    "  return tokenized_sentences, token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dict(dict1, dict2):\n",
    "  \"\"\"Return a new dictionary with the content of dict1 and dict2\"\"\"  \n",
    "  res = {**dict1, **dict2}\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    \"\"\"Retun a list of tokenized sentences and a word frequency dictionary\"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    categories = [\"/businessEconomy\", \"/lifestyle\", \"/politics\", \"/technology\"]\n",
    "\n",
    "    tokenized_sentences = []\n",
    "    word_freq_dict = dict()\n",
    "\n",
    "    for category in categories:\n",
    "        docs_list = os.listdir(cwd+category)\n",
    "        for docs in docs_list:\n",
    "            doc_path = cwd+category+\"/\"+docs\n",
    "            tkn_sent, tkn_dict = load_data(doc_path)\n",
    "            tokenized_sentences += tkn_sent\n",
    "            word_freq_dict = merge_dict(word_freq_dict, tkn_dict)\n",
    "\n",
    "    return tokenized_sentences, word_freq_dict        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences, word_freq_dict = process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = tf.keras.utils.get_file(fname=\"tagged_tsv_gh\", origin=\"https://raw.githubusercontent.com/famrashel/idn-tagged-corpus/master/Indonesian_Manually_Tagged_Corpus_ID.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "length of lines is  276682\n<kalimat id=1>\nFalse\nKera\tNN\nFalse\nuntuk\tSC\nFalse\namankan\tVB\nFalse\npesta olahraga\tNN\nFalse\n</kalimat>\nFalse\n<kalimat id=2>\nFalse\nPemerintah\tNNP\nFalse\nkota\tNNP\nFalse\nDelhi\tNNP\nFalse\nmengerahkan\tVB\nFalse\nmonyet\tNN\nFalse\nuntuk\tSC\nFalse\n"
     ]
    }
   ],
   "source": [
    "# this is a tag with all the pos and the corresponding word as a list\n",
    "pos_tags = dict()\n",
    "train = []\n",
    "f = open(tagged_data, \"r\")\n",
    "lines = f.readlines()\n",
    "print(\"length of lines is \", len(lines))\n",
    "for line in lines[:13]:\n",
    "    line = line.strip()\n",
    "    # print(f\"line[:2] is {line[:2]}\")\n",
    "    print(line)\n",
    "    stop = str(\"</kalimat>\")\n",
    "    print(line is stop)\n",
    "    if line[:1] is \"<\":\n",
    "        # instantiate a new tagged entry\n",
    "        tagged_entry = []\n",
    "    # elif line[:2] is \"</\":\n",
    "    elif line is stop:\n",
    "        # print(line)\n",
    "        train.append(tagged_entry)\n",
    "    else:\n",
    "        lst = line.split(\"\\t\")\n",
    "        entry = (lst[0], lst[1])\n",
    "        tagged_entry.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "len(train)"
   ]
  }
 ]
}