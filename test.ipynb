{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "\n",
    "# for tokenizing\n",
    "nltk.download('punkt')\n",
    "\n",
    "# for POS tagging\n",
    "from nltk.tag.perceptron import AveragedPerceptron\n",
    "from nltk.tag.perceptron import PerceptronTagger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "  \"\"\"Return a list of tokenized sentence, and\n",
    "      a dictionary of frequency of word\n",
    "    \n",
    "    Args:\n",
    "      path: the path of the file (in txt) to parse and tokenize\n",
    "  \"\"\"\n",
    "  f = open(path, \"r\")\n",
    "  text = f.read()\n",
    "  token_dict = dict()\n",
    "\n",
    "  # documents -> list of sentences\n",
    "  sentences_lst = nltk.sent_tokenize(text)\n",
    "\n",
    "  # list of sentences -> list of tokenized sentences\n",
    "  tokenized_sentences = []\n",
    "  for sentence in sentences_lst:\n",
    "    tokenized = nltk.word_tokenize(sentence)\n",
    "    tokenized_sentences.append(tokenized)\n",
    "    for token in tokenized:\n",
    "      if token in token_dict:\n",
    "        token_dict[token] += 1\n",
    "      else:\n",
    "        token_dict[token] = 1\n",
    "\n",
    "  return tokenized_sentences, token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dict(dict1, dict2):\n",
    "  \"\"\"Return a new dictionary with the content of dict1 and dict2\"\"\"  \n",
    "  res = {**dict1, **dict2}\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    \"\"\"Retun a list of tokenized sentences and a word frequency dictionary\"\"\"\n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "    categories = [\"/businessEconomy\", \"/lifestyle\", \"/politics\", \"/technology\"]\n",
    "\n",
    "    tokenized_sentences = []\n",
    "    word_freq_dict = dict()\n",
    "\n",
    "    for category in categories:\n",
    "        docs_list = os.listdir(cwd+category)\n",
    "        for docs in docs_list:\n",
    "            doc_path = cwd+category+\"/\"+docs\n",
    "            tkn_sent, tkn_dict = load_data(doc_path)\n",
    "            tokenized_sentences += tkn_sent\n",
    "            word_freq_dict = merge_dict(word_freq_dict, tkn_dict)\n",
    "\n",
    "    return tokenized_sentences, word_freq_dict        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences, word_freq_dict = process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = tf.keras.utils.get_file(fname=\"tagged_tsv_gh\", origin=\"https://raw.githubusercontent.com/famrashel/idn-tagged-corpus/master/Indonesian_Manually_Tagged_Corpus_ID.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PrepareData():\n",
    "    def __init__(self, path):\n",
    "        self.tagged_data = path\n",
    "        self.pos_tags = dict()\n",
    "        self.data = []\n",
    "        self.pos_tags_set = set()\n",
    "\n",
    "    def load(self):\n",
    "        f = open(self.tagged_data, \"r\")\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line[:2] == \"</\":\n",
    "                # end of a sentence\n",
    "                self.data.append(tagged_entry)\n",
    "            elif line[:1] == \"<\":\n",
    "                # instantiate a new tagged entry\n",
    "                tagged_entry = []\n",
    "            else:\n",
    "                lst = line.split(\"\\t\")\n",
    "                word = lst[0]\n",
    "                tag = lst[1]\n",
    "\n",
    "                # add entry to tagged_entry to build tagged sentence\n",
    "                entry = (word, tag)\n",
    "                tagged_entry.append(entry)\n",
    "                \n",
    "                # add the tag and word to pos_tags\n",
    "                if tag not in self.pos_tags:\n",
    "                    self.pos_tags[tag] = set(word)\n",
    "                else:\n",
    "                    self.pos_tags[tag].add(word)\n",
    "\n",
    "        self.pos_tags_set = set(self.pos_tags.keys())\n",
    "\n",
    "    def split_test_train(self, ratio=0.8):\n",
    "        \"\"\"Split the data into test set and train set. Default ratio is 80% train and 20% test\"\"\"\n",
    "        num_train_data = len(self.data)\n",
    "        choose = math.floor(num_train_data * (1 - ratio))\n",
    "        test_data = random.sample(self.data, choose)\n",
    "        train_data = list(filter(lambda i: i not in test_data, self.data))\n",
    "        return train_data, test_data\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_data = PrepareData(tagged_data)\n",
    "prepare_data.load()\n",
    "train_data, test_data = prepare_data.split_test_train()\n",
    "data = prepare_data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_modal = set([\"akan\", \"ingin\", \"mau\", \"telah\", \"sudah\", \"pernah\", \"masih\", \"bisa\", \"dapat\", \"boleh\", \"ikut\", \"harus\", \"suka\", \"senang\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modal(data, default_modal):\n",
    "    \"\"\"Return the sentences that contains auxiliary verb from the prepared data\n",
    "        and the set of modal / auxiliary words\n",
    "\n",
    "        Args:\n",
    "            train_data: from the corpus of tagged words\n",
    "            default_modal: a set of default modal words that will be used \n",
    "                to detect possible modal that is not labeled as modal\n",
    "\n",
    "        Return data will be a dictionary:\n",
    "            key: modal words\n",
    "            values: list of sentences containing the modal words (no repeat)\n",
    "    \"\"\"\n",
    "    f = open(\"report.txt\", \"w\")\n",
    "    modal_sent_dict = dict()\n",
    "    num_misclassified_modal = 0\n",
    "    for sentence in data:\n",
    "        for entry in sentence:\n",
    "            word = entry[0]\n",
    "            tag = entry[1]\n",
    "            if tag == \"MD\":\n",
    "                if word not in modal_sent_dict:\n",
    "                    modal_sent_dict[word] = [sentence]\n",
    "                elif sentence not in modal_sent_dict[word]:\n",
    "                    modal_sent_dict[word].append([sentence])    \n",
    "            elif word in default_modal:\n",
    "                f.write(f\"the word {word} is a modal but not tagged as MD\\n\")\n",
    "                f.write(f\"\\t{word} is classified as {tag}\\n\")\n",
    "                f.write(f\"\\tin sentence {sentence}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "                num_misclassified_modal += 1\n",
    "    f.close()\n",
    "    print(f\"there are {num_misclassified_modal} instances where modal words that are not classified as modal\")\n",
    "    print(f\"there are {len(modal_sent_dict.keys())} modal words\")\n",
    "    print(f\"{modal_sent_dict.keys()}\")\n",
    "    # for each modal words in the key set, make the value to a set then back to list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "there are 131 instances where modal words that are not classified as modal\nthere are 39 modal words\ndict_keys(['akan', 'bisa', 'harus', 'telah', 'sudah', 'baru', 'mulai', 'pernah', 'masih', 'sempat', 'sedang', 'tengah', 'mau', 'sepertinya', 'perlu', 'boleh', 'dapat', 'mungkin', 'bakal', 'kemungkinan', 'sedikit', 'Baru', 'Bisa', 'hendak', 'Mungkin', 'Sudah', 'barangkali', 'Harus', 'Pernah', 'Masih', 'Akan', 'guna', 'Dapat', 'usah', 'mesti', 'kan', 'Telah', 'lagi', 'Perlu'])\n"
     ]
    }
   ],
   "source": [
    "get_modal(data, default_modal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_tagger = PerceptronTagger(load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS IF YOU WANT TO RETRAIN\n",
    "perceptron_tagger.train(train_data, save_loc=\"trained_model_split_80.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD THE SAVED MODEL\n",
    "perceptron_tagger.load(\"trained_model.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_auxiliary(tokenized_sentences):\n",
    "    \"\"\"Detect auxiliary verbs or modal verbs'\"\""
   ]
  }
 ]
}